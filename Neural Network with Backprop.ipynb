{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "182b2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and magics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e554918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(formula, x_range):  \n",
    "    x = np.array(x_range)  \n",
    "    y = eval(formula)\n",
    "    # plt.plot(x, y) \n",
    "    #return axis.plot(x, y, color='red')\n",
    "    return x, y\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea7d7b",
   "metadata": {},
   "source": [
    "## The Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b72225d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the ReLU activation function\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fe415950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spiral_data(points, classes):\n",
    "    X = np.zeros((points*classes, 2))\n",
    "    y = np.zeros((points*classes), dtype='int')\n",
    "    \n",
    "    \n",
    "    for class_number in range(classes):\n",
    "        ix = range(points*class_number, points*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, points)  # radius\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dab6008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "\n",
    "    # Get unique categories in the data  [0, 0, 0,] ---. [[0, 1], [1, 0]]\n",
    "    unique_categories = np.unique(data)\n",
    "\n",
    "    # Create an empty array to hold the one-hot encoding\n",
    "    one_hot_encoded = np.zeros((len(data), len(unique_categories)))\n",
    "\n",
    "    # Fill in the one-hot encoding\n",
    "    for i, category in enumerate(data):\n",
    "        one_hot_encoded[i, np.where(unique_categories == category)] = 1\n",
    "\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f5a115b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(training_data, training_labels, o1_raw, o1, output_weights, o2):\n",
    "    # Output bias\n",
    "    # m = training_labels.size\n",
    "    # dSSR = -2 * (training_labels - o2)\n",
    "    # dW2 = 1/m * dSSR.dot(o1.T)\n",
    "    # db2 = 1/m * np.sum(dSSR, axis=1, keepdims=True)\n",
    "\n",
    "    m = training_labels.size\n",
    "    dSSR = -2 * (training_labels - o2)\n",
    "    dW2 = dSSR.dot(o1.T)\n",
    "    db2 = np.sum(dSSR, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    # Hidden\n",
    "    dZ1 = output_weights.T.dot(dSSR) * relu_prime(o1_raw)\n",
    "    dW1 = 1/m * dZ1.dot(training_data.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f446585",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "51811092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size- how MANY of the samples we want to show at a time\n",
    "\n",
    "# set seed of random initializations\n",
    "np.random.seed(2)\n",
    "\n",
    "X = [[1,0],\n",
    "     [0,1],\n",
    "     [-1,0],\n",
    "     [0,-1],\n",
    "     [0.5,0.5],\n",
    "     [-0.5,0.5],\n",
    "     [0.5, -0.5],\n",
    "     [-0.5,-0.5]]\n",
    "\n",
    "t = [1,1,1,1,0,0,0,0]\n",
    "t_one_hot_encoded = [[0,1],\n",
    "                     [0,1],\n",
    "                     [0,1],\n",
    "                     [0,1],\n",
    "                     [1,0],\n",
    "                     [1,0],\n",
    "                     [1,0],\n",
    "                     [1,0]]\n",
    "\n",
    "X = np.array(X)\n",
    "t = np.array(t)\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        # initialize weights as inputs x num neurons instead of other way, so that when we do matrix operations, we don't have to transpose\n",
    "        #self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) # to keep random values constrained between -1 and 1\n",
    "        self.weights = np.random.uniform(-0.5, 0.5, (n_neurons, n_inputs))\n",
    "        self.biases = np.zeros((n_neurons, 1))\n",
    "    #def forward(self, inputs):\n",
    "    #    self.output = self.weights.dot(inputs) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    def derivative(self, netValue): # netValue is an array of values\n",
    "        self.derivative_value = (netValue > 0)\n",
    "        return netValue > 0\n",
    "    \n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.output = 1/(1+np.exp(-inputs))\n",
    "    def derivative(self, netValue):\n",
    "        self.derivative_value = netValue *(1-netValue)\n",
    "        return self.derivative_value\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) # we want max values WITHIN a batch, not among other batches\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "class Activation_TanH:\n",
    "    def forward(self, inputs): # where input is a SCALER, or the NET (sum of weights and inputs)\n",
    "        self.output = np.tanh(inputs)\n",
    "    def derivative(self, netValue): # netValue is an array of values\n",
    "        self.derivative_value = 0.5 * (1 - np.square(netValue))\n",
    "        return self.derivative_value\n",
    "        \n",
    "        \n",
    "class Loss:\n",
    "    def calculate(self, output, y): # output is output of model, y is INTENDED target values\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses) # or the mean loss of every sample in batch\n",
    "        return data_loss\n",
    "    \n",
    "# basically, for the below, the Categorical Cross Entropy for a one-hot encoded target vector is just -log(prediction) at that one hot encoded vector\n",
    "class Loss_CategoricalCrossEntropy(Loss): # will inherit from base loss class\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7) # because -log(0) = infinity, so we never want exactly 0\n",
    "        \n",
    "        if len(y_true.shape) == 1: # meaning the target vector is something passed in like [1, 2, 0, 1] where lets say index 0 tells you for batch 0, class 1 is the predicted class\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true] # range(samples) means we want ALL the batches, and y_true tells us for each batch, which sample to grab (corresponding to target class)\n",
    "        elif len(y_true.shape) == 2: # if one-hot encoded vectors are passed for each batch, like [ [1,0,0], [0,1,0],... ]\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true.T, axis=1) # for each batch, multiply samples by one-hot encoded vecotr, to only keep data corresponding to target class, and then sum to get only the values we want, not all the 0's\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods # is the vector of values corresponding to -log( sample within a batch corresponding to target class, for each sample within a batch)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "1fa3e3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "[[-0.00000000e+00 -1.81405520e-05 -1.73466528e-03 ...  9.97979335e-01\n",
      "   8.80843050e-01  9.99107962e-01]\n",
      " [ 0.00000000e+00  1.00083661e-03  9.99474056e-04 ...  6.10337564e-03\n",
      "   4.71290272e-01  4.22289048e-02]]\n",
      "(2, 2000)\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "num_points = 1000\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "X, labels = spiral_data(num_points, num_classes)\n",
    "\n",
    "X = X.T\n",
    "\n",
    "t = one_hot_encode(labels)\n",
    "\n",
    "print(len(np.array(t)))\n",
    "\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(t)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "423d42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18047218  0.12673595]\n",
      " [ 0.4027876   0.26632841]\n",
      " [ 0.40484565 -0.31309311]\n",
      " [ 0.07301932  0.39097352]\n",
      " [ 0.14459498  0.42747351]\n",
      " [-0.45108355  0.014763  ]\n",
      " [ 0.41902219  0.14581801]\n",
      " [-0.20059297  0.43925547]]\n",
      "[[ 0.25452902  0.05429244 -0.45887834  0.08294022 -0.0452265   0.21835405\n",
      "   0.09125661  0.21703291]]\n"
     ]
    }
   ],
   "source": [
    "# Define Network\n",
    "\n",
    "# Define Layers\n",
    "dense1 = Layer_Dense(2,8) # input is 2 because input data is just (x,y)\n",
    "activation1 = Activation_TanH()\n",
    "\n",
    "dense2 = Layer_Dense(8,1)\n",
    "activation2 = Activation_TanH()\n",
    "\n",
    "print(dense1.weights)\n",
    "print(dense2.weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "ebd92277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.3587803912191983\n",
      "Training MSE: 0.35873211146589573\n",
      "Training MSE: 0.3587635612587303\n",
      "Training MSE: 0.35880221316201844\n",
      "Training MSE: 0.3588447506737724\n",
      "Training MSE: 0.35889023945681103\n",
      "Training MSE: 0.35893795610712004\n",
      "Training MSE: 0.35898730014779834\n",
      "Training MSE: 0.35903777443475043\n",
      "Training MSE: 0.35908896937922896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------- Hyperparameters ------------ #\n",
    "\n",
    "eta = 0.005 # define learning rate\n",
    "EPOCHS = 10\n",
    "batch_size = 2\n",
    "\n",
    "# ------------------------------------------------- #\n",
    "\n",
    "# Define data\n",
    "\n",
    "#print('initial weights are:', dense1.weights, dense2.weights)\n",
    "\n",
    "#print('data is', X[:10])\n",
    "#print(X.shape)\n",
    "\n",
    "index = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    error2 = 0\n",
    "    \n",
    "    for i in range(1000):\n",
    "\n",
    "        \n",
    "        first = index\n",
    "        second = index+batch_size\n",
    "\n",
    "\n",
    "        if (first == num_points):\n",
    "            first = index % num_points\n",
    "            second = (index+batch_size) % num_points\n",
    "            index = index % num_points\n",
    "\n",
    "    # ---------------- For batch training -------------------#\n",
    "\n",
    "        # Grab the batch of sampels / labels from data\n",
    "        #input_data = np.array( X[first:second] )\n",
    "        #target_OHE = np.array( t[first:second] )\n",
    "    # --------------------------------------------------------#\n",
    "    \n",
    "        input_data = np.array(X)\n",
    "        target_OHE = np.array(labels)\n",
    "\n",
    "        m = len(np.array(labels))\n",
    "\n",
    "        # Run Network\n",
    "        o1_raw = dense1.weights.dot(input_data) + dense1.biases\n",
    "        activation1.forward(o1_raw)\n",
    "\n",
    "        o2_raw = dense2.weights.dot(activation1.output) + dense2.biases\n",
    "        activation2.forward(o2_raw) # output of neural network\n",
    "\n",
    "\n",
    "        # ------------------- Loss for Categorical Entropy -----------------#\n",
    "        #loss_function = Loss_CategoricalCrossEntropy()\n",
    "        #loss = loss_function.calculate(activation2.output, target_OHE)\n",
    "        # ------------------------------------------------------------------- #\n",
    "        \n",
    "        # ---------------------------Accuracy -------------------------------#\n",
    "        #num_correct = 0\n",
    "        #if i % 1000 == 0:\n",
    "        #print('the loss of iteration', i, 'is:', loss)\n",
    "        #  for j in range(len(target_OHE)):\n",
    "        #      if (activation2.output[0][j] > 0 and target_OHE[j] == 1) or (activation2.output[0][j] < 0 and target_OHE[j] == -1):\n",
    "        #          num_correct += 1\n",
    "\n",
    "        #  print('the accuracy of iteration', i, 'is:', num_correct / len(np.array(target_OHE)))   \n",
    "\n",
    "        \n",
    "        for j in range(m):\n",
    "            error2 += (labels[j]-activation2.output[0][j])**2\n",
    "        error2 = error2/m\n",
    "\n",
    "        \n",
    "\n",
    "        # BP for output layer\n",
    "        error = target_OHE - activation2.output\n",
    "\n",
    "        #softmax_derivative = target_OHE.T - activation2.output\n",
    "        layer2_derivative = activation2.derivative(o2_raw)\n",
    "\n",
    "\n",
    "        output_error = -error * layer2_derivative * activation2.output\n",
    "\n",
    "        output_weight_update = output_error.dot(activation1.output.T)\n",
    "\n",
    "\n",
    "        temp_dense2_weights = dense2.weights + eta * output_weight_update\n",
    "        temp_dense2_biases = dense2.biases + eta * np.sum(output_error, axis=1, keepdims=True)\n",
    "\n",
    "        # BP for hidden layer\n",
    "        scaled_by_weights = dense2.weights.T.dot(output_error) \n",
    "\n",
    "        overall_error = scaled_by_weights * activation1.derivative(o1_raw)\n",
    "\n",
    "        weight_updates = overall_error.dot(input_data.T)\n",
    "\n",
    "        # update weight parameters\n",
    "        dense1.weights = dense1.weights + eta * weight_updates\n",
    "        dense2.weights = temp_dense2_weights\n",
    "\n",
    "        # Update bias parameters\n",
    "        dense1.biases = dense1.biases + eta * np.sum(overall_error, axis=1, keepdims=True)\n",
    "        dense2.biases = temp_dense2_biases\n",
    "\n",
    "        # ------------ For Batch Training ---------#\n",
    "        # Update counter\n",
    "        #index += batch_size\n",
    "        # ---------------------------------------- #\n",
    "\n",
    "        \n",
    "    MSE = error2\n",
    "    print(f'Training MSE: {MSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fed5b9",
   "metadata": {},
   "source": [
    "## Code With Other BackProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9bce5e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size- how MANY of the samples we want to show at a time\n",
    "\n",
    "# set seed of random initializations\n",
    "np.random.seed(2)\n",
    "\n",
    "X = [[1,0],\n",
    "     [0,1],\n",
    "     [-1,0],\n",
    "     [0,-1],\n",
    "     [0.5,0.5],\n",
    "     [-0.5,0.5],\n",
    "     [0.5, -0.5],\n",
    "     [-0.5,-0.5]]\n",
    "\n",
    "t = [1,1,1,1,0,0,0,0]\n",
    "t_one_hot_encoded = [[0,1],\n",
    "                     [0,1],\n",
    "                     [0,1],\n",
    "                     [0,1],\n",
    "                     [1,0],\n",
    "                     [1,0],\n",
    "                     [1,0],\n",
    "                     [1,0]]\n",
    "\n",
    "X = np.array(X)\n",
    "t = np.array(t)\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        # initialize weights as inputs x num neurons instead of other way, so that when we do matrix operations, we don't have to transpose\n",
    "        #self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) # to keep random values constrained between -1 and 1\n",
    "        self.weights = np.random.uniform(-0.5, 0.5, (n_inputs, n_neurons))\n",
    "        self.biases = np.zeros((n_neurons, 1))\n",
    "    def forward(self, inputs):\n",
    "        self.output = self.weights.dot(inputs) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    def derivative(self, netValue): # netValue is an array of values\n",
    "        self.derivative_value = (netValue > 0)\n",
    "        return netValue > 0\n",
    "    \n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def derivative(self, netValue):\n",
    "        return x *(1-x)\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) # we want max values WITHIN a batch, not among other batches\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "class Activation_TanH:\n",
    "    def forward(self, inputs): # where input is a SCALER, or the NET (sum of weights and inputs)\n",
    "        self.output = np.tanh(inputs)\n",
    "    def derivative(self, netValue): # netValue is an array of values\n",
    "        self.derivative_value = 0.5 * (1 - np.square(netValue))\n",
    "        return self.derivative_value\n",
    "        \n",
    "        \n",
    "class Loss:\n",
    "    def calculate(self, output, y): # output is output of model, y is INTENDED target values\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses) # or the mean loss of every sample in batch\n",
    "        return data_loss\n",
    "    \n",
    "# basically, for the below, the Categorical Cross Entropy for a one-hot encoded target vector is just -log(prediction) at that one hot encoded vector\n",
    "class Loss_CategoricalCrossEntropy(Loss): # will inherit from base loss class\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7) # because -log(0) = infinity, so we never want exactly 0\n",
    "        \n",
    "        if len(y_true.shape) == 1: # meaning the target vector is something passed in like [1, 2, 0, 1] where lets say index 0 tells you for batch 0, class 1 is the predicted class\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true] # range(samples) means we want ALL the batches, and y_true tells us for each batch, which sample to grab (corresponding to target class)\n",
    "        elif len(y_true.shape) == 2: # if one-hot encoded vectors are passed for each batch, like [ [1,0,0], [0,1,0],... ]\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true.T, axis=1) # for each batch, multiply samples by one-hot encoded vecotr, to only keep data corresponding to target class, and then sum to get only the values we want, not all the 0's\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods # is the vector of values corresponding to -log( sample within a batch corresponding to target class, for each sample within a batch)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4a1858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[[-0.00000000e+00  0.00000000e+00]\n",
      " [-1.81405520e-05  1.00083661e-03]\n",
      " [-1.73466528e-03  9.99474056e-04]\n",
      " ...\n",
      " [ 9.97979335e-01  6.10337564e-03]\n",
      " [ 8.80843050e-01  4.71290272e-01]\n",
      " [ 9.99107962e-01  4.22289048e-02]]\n",
      "(2000, 2)\n",
      "[1 1 1 1 0 0 0 0]\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "num_points = 1000\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "X, labels = spiral_data(num_points, num_classes)\n",
    "#t = one_hot_encode(labels)\n",
    "\n",
    "print(len(np.array(t)))\n",
    "\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(t)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55cf0658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01804722  0.0126736   0.04027876  0.02663284]\n",
      " [ 0.04048456 -0.03130931  0.00730193  0.03909735]]\n",
      "[[ 0.0144595 ]\n",
      " [ 0.04274735]\n",
      " [-0.04510836]\n",
      " [ 0.0014763 ]]\n"
     ]
    }
   ],
   "source": [
    "# Define Network\n",
    "\n",
    "# Define Layers\n",
    "dense1 = Layer_Dense(2,4) # input is 2 because input data is just (x,y)\n",
    "activation1 = Activation_Sigmoid()\n",
    "\n",
    "dense2 = Layer_Dense(4,1)\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "print(dense1.weights)\n",
    "print(dense2.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e86f2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid activation \n",
    "def sig(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "#derivative of sigmoid activation\n",
    "def dsig(x):\n",
    "    return x *(1-x)\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b303140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = np.random.uniform(-0.5,0.5,(2,4))\n",
    "L2 = np.random.uniform(-0.5,0.5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "828a844f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.2059778874908301\n",
      "Training MSE: 0.17710006990982471\n",
      "Training MSE: 0.13252270099136024\n",
      "Training MSE: 0.07613486919158846\n",
      "Training MSE: 0.029986832968767377\n",
      "Training MSE: 0.009011028711387023\n",
      "Training MSE: 0.002762177842354298\n",
      "Training MSE: 0.0010139654368932426\n",
      "Training MSE: 0.0004483545968355276\n",
      "Training MSE: 0.00022995256417176547\n"
     ]
    }
   ],
   "source": [
    "# ----------- Hyperparameters ------------ #\n",
    "\n",
    "eta = 0.001 # define learning rate\n",
    "EPOCHS = 10\n",
    "batch_size = 2\n",
    "\n",
    "# ------------------------------------------------- #\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    error2 = 0\n",
    "    \n",
    "    for n in range(num_points):\n",
    "        #Forward Pass (no bias)\n",
    "        net_0 = np.sum(L1.T*X[n,:],axis=1) #more explict way to write dot product\n",
    "        fnet_0 = sig(net_0)\n",
    "        net_1 = np.sum(L2*fnet_0)\n",
    "        fnet1 = sig(net_1)\n",
    "\n",
    "        error2 += (labels[n]-fnet1)**2\n",
    "        \n",
    "        #Backwards Pass \n",
    "        error = labels[n]- fnet1\n",
    "        gradient2 = -error*dsig(net_1)*fnet_0 #gradient for L2\n",
    "        gradient1 = gradient2*L2*dsig(net_0) #gradient for L1\n",
    "       \n",
    "\n",
    "        L1[0,:] += gradient1*eta*X[n,0]\n",
    "        L1[1,:] += gradient1*eta*X[n,1]\n",
    "        L2 += gradient2*eta\n",
    "        \n",
    "    MSE = error2/num_points\n",
    "    print(f'Training MSE: {MSE}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf58bc",
   "metadata": {},
   "source": [
    "## BackProp using Strictly Formulas From Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "28efcc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size- how MANY of the samples we want to show at a time\n",
    "\n",
    "# set seed of random initializations\n",
    "np.random.seed(2)\n",
    "\n",
    "X = [[1,0],\n",
    "     [0,1],\n",
    "     [-1,0],\n",
    "     [0,-1],\n",
    "     [0.5,0.5],\n",
    "     [-0.5,0.5],\n",
    "     [0.5, -0.5],\n",
    "     [-0.5,-0.5]]\n",
    "\n",
    "t = [1,1,1,1,0,0,0,0]\n",
    "t_one_hot_encoded = [[0,1],\n",
    "                     [0,1],\n",
    "                     [0,1],\n",
    "                     [0,1],\n",
    "                     [1,0],\n",
    "                     [1,0],\n",
    "                     [1,0],\n",
    "                     [1,0]]\n",
    "\n",
    "X = np.array(X)\n",
    "t = np.array(t)\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        # initialize weights as inputs x num neurons instead of other way, so that when we do matrix operations, we don't have to transpose\n",
    "        #self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) # to keep random values constrained between -1 and 1\n",
    "        self.weights = np.random.uniform(-0.5, 0.5, (n_inputs, n_neurons))\n",
    "        self.biases = np.zeros((n_neurons, 1))\n",
    "    def forward(self, inputs):\n",
    "        self.output = self.weights.dot(inputs) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    def derivative(self, netValue): # netValue is an array of values\n",
    "        self.derivative_value = (netValue > 0)\n",
    "        return netValue > 0\n",
    "    \n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.output = 1/(1+np.exp(-inputs))\n",
    "    def derivative(self, netValue):\n",
    "        self.derivative = netValue *(1-netValue)\n",
    "        return netValue *(1-netValue)\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) # we want max values WITHIN a batch, not among other batches\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "class Activation_TanH:\n",
    "    def forward(self, inputs): # where input is a SCALER, or the NET (sum of weights and inputs)\n",
    "        self.output = np.tanh(inputs)\n",
    "    def derivative(self, netValue): # netValue is an array of values\n",
    "        self.derivative_value = 0.5 * (1 - np.square(netValue))\n",
    "        return self.derivative_value\n",
    "        \n",
    "        \n",
    "class Loss:\n",
    "    def calculate(self, output, y): # output is output of model, y is INTENDED target values\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses) # or the mean loss of every sample in batch\n",
    "        return data_loss\n",
    "    \n",
    "# basically, for the below, the Categorical Cross Entropy for a one-hot encoded target vector is just -log(prediction) at that one hot encoded vector\n",
    "class Loss_CategoricalCrossEntropy(Loss): # will inherit from base loss class\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7) # because -log(0) = infinity, so we never want exactly 0\n",
    "        \n",
    "        if len(y_true.shape) == 1: # meaning the target vector is something passed in like [1, 2, 0, 1] where lets say index 0 tells you for batch 0, class 1 is the predicted class\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true] # range(samples) means we want ALL the batches, and y_true tells us for each batch, which sample to grab (corresponding to target class)\n",
    "        elif len(y_true.shape) == 2: # if one-hot encoded vectors are passed for each batch, like [ [1,0,0], [0,1,0],... ]\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true.T, axis=1) # for each batch, multiply samples by one-hot encoded vecotr, to only keep data corresponding to target class, and then sum to get only the values we want, not all the 0's\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods # is the vector of values corresponding to -log( sample within a batch corresponding to target class, for each sample within a batch)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a2d78578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00181373  0.00466481]\n",
      "-0.0018137297526784884\n"
     ]
    }
   ],
   "source": [
    "num_points = 1000\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "X, labels = spiral_data(num_points, num_classes)\n",
    "#t = one_hot_encode(labels)\n",
    "\n",
    "print(X[5])\n",
    "print(X[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "247698c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41902219  0.14581801 -0.20059297  0.43925547]\n",
      " [ 0.25452902  0.05429244 -0.45887834  0.08294022]]\n",
      "[[-0.0452265 ]\n",
      " [ 0.21835405]\n",
      " [ 0.09125661]\n",
      " [ 0.21703291]]\n"
     ]
    }
   ],
   "source": [
    "# Define Network\n",
    "\n",
    "# Define Layers\n",
    "dense1 = Layer_Dense(2,4) # input is 2 because input data is just (x,y)\n",
    "activation1 = Activation_Sigmoid()\n",
    "\n",
    "dense2 = Layer_Dense(4,1)\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "print(dense1.weights)\n",
    "print(dense2.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9ff8dc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00000000e+00  0.00000000e+00]\n",
      " [-1.81405520e-05  1.00083661e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "00ac0e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is [[-0.00000000e+00  0.00000000e+00]\n",
      " [-1.81405520e-05  1.00083661e-03]]\n",
      "dense.weights is [[ 0.41902219  0.14581801 -0.20059297  0.43925547]\n",
      " [ 0.25452902  0.05429244 -0.45887834  0.08294022]] and its shape is [[ 0.41902219  0.14581801 -0.20059297  0.43925547]\n",
      " [ 0.25452902  0.05429244 -0.45887834  0.08294022]]\n",
      "dense2.weights is [[-0.0452265 ]\n",
      " [ 0.21835405]\n",
      " [ 0.09125661]\n",
      " [ 0.21703291]] and its shape is [[-0.0452265 ]\n",
      " [ 0.21835405]\n",
      " [ 0.09125661]\n",
      " [ 0.21703291]]\n",
      "net_0 is [ 0.16191093  0.6566658   0.28601525 -0.08414073] and its shape is (4,)\n",
      "fnet_0 is [0.54038954 0.65851101 0.57102032 0.47897722] and its shape is (4,)\n",
      "(4, 1)\n",
      "(4, 2)\n",
      "net_1 is -4.950052474590725 and its shape is ()\n",
      "fnet_1 is 0.0070332206758568685 and its shape is ()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#Backwards Pass \u001b[39;00m\n\u001b[0;32m     37\u001b[0m error \u001b[38;5;241m=\u001b[39m labels[n]\u001b[38;5;241m-\u001b[39m activation2\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m---> 38\u001b[0m gradient2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39merror\u001b[38;5;241m*\u001b[39m\u001b[43mactivation2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mderivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mactivation2\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;66;03m#gradient for L2\u001b[39;00m\n\u001b[0;32m     39\u001b[0m gradient1 \u001b[38;5;241m=\u001b[39m gradient2\u001b[38;5;241m*\u001b[39mdense2\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m*\u001b[39mactivation1\u001b[38;5;241m.\u001b[39mderivative(net0) \u001b[38;5;66;03m#gradient for L1\u001b[39;00m\n\u001b[0;32m     42\u001b[0m dense1\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gradient1\u001b[38;5;241m*\u001b[39meta\u001b[38;5;241m*\u001b[39mX[:\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "# ----------- Hyperparameters ------------ #\n",
    "\n",
    "eta = 0.001 # define learning rate\n",
    "EPOCHS = 10\n",
    "batch_size = 2\n",
    "\n",
    "# ------------------------------------------------- #\n",
    "\n",
    "#for epoch in range(EPOCHS):\n",
    "for epoch in range(1):\n",
    "\n",
    "    error2 = 0\n",
    "    \n",
    "    #for n in range(num_points):\n",
    "    for n in range(1):\n",
    "        #Forward Pass (no bias)\n",
    "        print('input is', X[:2])\n",
    "        print('dense.weights is', dense1.weights, 'and its shape is', dense1.weights)\n",
    "        print('dense2.weights is', dense2.weights, 'and its shape is', dense2.weights)\n",
    "        net0 = dense1.weights.T.dot(X[:2])\n",
    "        activation1.forward(net0)\n",
    "        print('net_0 is', net_0, 'and its shape is', net_0.shape)\n",
    "        print('fnet_0 is', fnet_0, 'and its shape is', fnet_0.shape)\n",
    "        \n",
    "        print(dense2.weights.shape)\n",
    "        print(activation1.output.shape)\n",
    "        net1 = dense2.weights.T.dot(activation1.output)\n",
    "        activation2.forward(net1) \n",
    "        \n",
    "        print('net_1 is', net_1, 'and its shape is', net_1.shape)\n",
    "        print('fnet_1 is', fnet1, 'and its shape is', fnet1.shape)\n",
    "        \n",
    "\n",
    "        error2 += (labels[n]-activation2.output)**2\n",
    "        \n",
    "        #Backwards Pass \n",
    "        error = labels[n]- activation2.output\n",
    "        gradient2 = -error*activation2.derivative(net1)*activation2.output #gradient for L2\n",
    "        gradient1 = gradient2*dense2.weights*activation1.derivative(net0) #gradient for L1\n",
    "       \n",
    "\n",
    "        dense1.weights += gradient1*eta*X[:2]\n",
    "        dense1.weights += gradient1*eta*X[:2]\n",
    "        dense2.weights += gradient2*eta\n",
    "        \n",
    "    MSE = error2/num_points\n",
    "    print(f'Training MSE: {MSE}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48158c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd797f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
